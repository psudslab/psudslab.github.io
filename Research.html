<!DOCTYPE HTML>
<html>
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>PSU Data Science Lab</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="" />
	<meta name="keywords" content="" />
	<meta name="author" content="" />

  <!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">

	<link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Flexslider  -->
	<link rel="stylesheet" href="css/flexslider.css">
	<!-- Flaticons  -->
	<link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
	<!-- Owl Carousel -->
	<link rel="stylesheet" href="css/owl.carousel.min.css">
	<link rel="stylesheet" href="css/owl.theme.default.min.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body>
		<div id="colorlib-page">
			<div class="container-wrap">
			<a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><i></i></a>
			<aside id="colorlib-aside" role="complementary" class="border js-fullheight">
				<div class="text-center">
					<div class="author-img" style="background-image: url(images/ds_photo.jpg);"></div>
					<h1 id="colorlib-logo"><a href="index.html">PSU Data Science Lab</a></h1>
					<span class="position">
						<a href="https://www.psu.edu"><font color="grey">Pennsylvania State University</font></a>
					</span>
				</div>
	
				<nav id="colorlib-main-menu">
					<div>
						<ul>
							<li class="active"><a href="index.html">Home</a></li>
							<li><a href="Research.html" >Research</a></li>
							<li><a href="Publications.html">Publications</a></li>
							<li><a href=“Talk.html">Talks</a></li>
							<li><a href="People.html">People</a></li>
							<li><a href="Contact.html">Contact</a></li>
						</ul>
					</div>
				</nav>
				<br>

			<div class="text-center">
				<p><font color="red"><b>Acknowledgement</b></font></p>

				<div class="field field-name-field-image field-type-image field-label-hidden">
					<div class="field-items">
					  <div class="field-item even">
						<center>
						  <img src="images/NSF.svg.png" width=140 height=140  alt=""/>
						</center>
					  </div>
					</div>
				  </div>
				  <br>

				  <div class="field field-name-field-image field-type-image field-label-hidden">
					<div class="field-items">
					  <div class="field-item even">
						<center>
						  <img src="images/nih-nia.png" width=120 height=120  alt=""/>
						</center>
					  </div>
					</div>
				  </div>
				  <br>

				  <div class="field field-name-field-image field-type-image field-label-hidden">
					<div class="field-items">
					  <div class="field-item even">
						<center>
						  <img src="images/Sony.png" width=120 height=60  alt=""/>
						</center>
					  </div>
					</div>
				  </div>

				<div class="field field-name-field-image field-type-image field-label-hidden">
					<div class="field-items">
					  <div class="field-item even">
						<center>
						  <img src="images/Penn-State-University-Emblem.png" width=240 height=120  alt=""/>
						</center>
					  </div>
					</div>
				  </div>
				
			</div>
			

			<div class="colorlib-footer">
				<p><small>&copy; <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
Copyright &copy;<script>document.write(new Date().getFullYear());</script> All rights reserved | This template is made with <i class="icon-heart" aria-hidden="true"></i> by <a href="https://colorlib.com" target="_blank">Colorlib</a>
<!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. --> </span> 
<span>Last Update: Mar 20, 2023</span></small></p>
			</div>
			<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=uk7inqkcxD7zV_n2SjGvBUPL002nRyMsNt895CillEg&co=8ac4ed'></script>
		</aside>

		<div id="colorlib-main">
		
			
			<section class="colorlib-research" data-section="research">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3 animate-box">
							<p><br></p>
							<span class="heading-meta">Our Focus</span>
							<h2 class="colorlib-heading animate-box">Research Projects</h2>
						</div>
					</div>
					<p style="text-align:justify; text-justify:inter-ideograph;">Our research focuses on the design, analysis, and application of learning algorithms for various big data, including health data, text data, image data, and multimodal data.
					The primary goal of our research is to explore both principled methodologies and innovative applications 
					with highly practical performance that can be used to understand the overwhelmingly large and complex data collected from our daily life. 
					</p>
					<div class="row">
						<div class="col-md-12">
				         <div class="timeline-centered">
					         <article class="timeline-entry animate-box">
					            <div class="timeline-entry-inner">

					               <div class="timeline-icon color-1">
					                  <i class="icon-pen2"></i>
					               </div>

					               <div class="timeline-label">
					                  <h2><a href="#">NSF IIS -- <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2238275&HistoricalAwards=false"><font color="blue"></font>Career: Automated Multimodal Learning for Healthcare</font></a>, 2023-2028, $555,752</a></h2>
					                  <p style="text-align:justify; text-justify:inter-ideograph;">Multimodal learning is one of the central tasks of artificial intelligence (AI), 
										which aims to effectively fuse and model multimodal data to gain a better understanding of the world around us. 
										Many multimodal fusion strategies have been proposed, ranging from manually designed policies to 
										advanced automated machine learning (AutoML)-based approaches. Although AutoML-based solutions outperform handcrafted ones, 
										they are still far from optimal due to their lack of generalizability in model design and failure to account 
										for the unique characteristics of multimodal data. This project takes the multimodal healthcare predictive 
										modeling task as a representative example, aiming to discover and identify the optimal way to fuse 
										multimodal data via a new learning paradigm, i.e., automated multimodal learning, with minimal human interventions. 
										The success of this project will yield new fundamental knowledge in various fields, including automated machine learning, 
										multimodal deep learning, and healthcare predictive modeling. The new automated multimodal learning paradigm will 
										revolutionize multimodal data mining by automatically searching for new and complex yet optimal fusion strategies from the data, 
										potentially motivating researchers and domain experts to understand the multimodal data better. 
										In addition, recognizing unique research challenges posed by the unique nature of multimodal data in the healthcare 
										domain and providing customized solutions will advance the research of healthcare predictive modeling significantly.										
									</p>
									  <p style="text-align:justify; text-justify:inter-ideograph;">
										To meet these goals, the investigator proposes to equip automated multimodal learning with the ability to model the unique 
										challenges of multimodal health data, including data size variety, noise, and missing modalities. 
										The investigator also proposes to validate the proposed research for different multimodal fusion tasks 
										in healthcare informatics and beyond and gather feedback from experts to refine the proposed research. 
										The results of this project will provide a needed paradigm shift toward automated multimodal data fusion, 
										impacting a broad range of research fields, including machine learning, data mining, and healthcare informatics. 
										The proposed research will also make an enduring contribution to multimodal predictive modeling in clinical practice and other domains. 
										The generated data, source codes, and software tools will be made available to researchers worldwide. 
										The open platform will expedite research, enhance global collaborations in this field, and provide longstanding value for academia, 
										healthcare organizations, and health industries. The proposed education plan will help to ensure that graduates are well equipped 
										to design and evaluate machine learning solutions and cultivate K-12 students' interest in computer science and informatics. 
										It will also lead to a more diverse population of undergraduate research assistants and enhance collaboration and networking among graduate students.
									  </p>
					               </div>
					            </div>
					         </article>


					         <article class="timeline-entry animate-box" >
					            <div class="timeline-entry-inner">
					               <div class="timeline-icon color-2">
					                  <i class="icon-pen2"></i>
					               </div>
					               <div class="timeline-label">
					               	<h2><a href="#">NIH R01 -- <a href="https://reporter.nih.gov/search/MiaY_SSBoEWCzHMZ3OY2dg/project-details/10437992"><font color="blue"></font>SCH: AI-Enhanced Multimodal Sensor-on-a-chip for Alzheimer's Disease Detection</font></a>, 2022-2026, $1.2M</a></h2>
					                  <p style="text-align:justify; text-justify:inter-ideograph;"><a href="NIHR01AG077016.html">[Project Page]</a> We propose a new research paradigm aimed at addressing scientific questions in both biosensing and machine learning for 
										the early prediction of Alzheimer's disease (AD), and at solving a grand challenge in the identification of minimally-invasive 
										AD biomarkers in tear, saliva, and blood. Our goal is to develop a novel and minimally-invasive system that integrates a 
										multimodal biosensing platform and a machine learning framework, which synergistically work together to significantly 
										enhance the detection accuracy. The program will pioneer a novel Multimodal Optical, Mechanical, Electrochemical Nano-sensor 
										with Twodimensional material Amplification (MOMENTA) platform for sensitive and selective detection of AD biomarkers. 
										The sensor outputs are used for training the new Hierarchical Multimodal Machine Learning (HMML) framework, which not 
										only automatically integrates the heterogeneous data from different modalities but also ranks the importance of different 
										biosensors and biomarkers for AD prediction. Moreover, the framework is able to identify potential new biomarkers based on a 
										statistical analysis of the learned weights on the input signals and provide feedback information to further improve the 
										MOMENTA platform design. This interdisciplinary research brings together materials scientists who create new twodimensional (2D) 
										material platforms for sensor enhancement, nanotechnology and device experts who advance chip-scale sensor platforms, 
										data scientists who analyze data with machine learning methods to target early prediction of AD, and AD experts who help to 
										identify potentially new AD biomarkers. The machine-learning-enhanced multi-modal sensor system will not only offer major 
										performance boost compared to state-of-the-art, but also yield critical insights on new biomarker discovery for AD diagnosis at an early stage.
					                  </p>
					               </div>
					            </div>
					         </article>

					         <article class="timeline-entry animate-box">
					            <div class="timeline-entry-inner">
					               <div class="timeline-icon color-3">
					                  <i class="icon-pen2"></i>
					               </div>
					               <div class="timeline-label">
					               	<h2><a href="#"><a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2212323&HistoricalAwards=false"><font color="blue"></font>NSF SaTC -- Understanding and Mitigating the Security Risks of AutoML</font></a>, 2022-2024, $500K</a></h2>
					                  <p style="text-align:justify; text-justify:inter-ideograph;">Automated machine learning (AutoML) represents a new machine learning paradigm that automates the pipeline from raw data to deployable models, 
										enabling a much wider range of people to use machine learning techniques. However, each stage of this pipeline is subject to malicious attacks, 
										which can lead to inaccurate or vulnerable models. This project’s goal is to understand how both the technologies underlying AutoML and the ways 
										it is adopted change security risks around machine learning and how possible defenses to them change when using AutoML. The success of this 
										project will not only improve the security of AutoML but also promote more principled practices of building and operating machine learning 
										systems in general, while contributing to knowledge in the areas of security, machine learning, and human-computer interaction.
									</p>
									<p style="text-align:justify; text-justify:inter-ideograph;">
										The project has three main sub-goals: accounting for the full spectrum of security risks that arise around AutoML; understanding the fundamental 
										factors that drive such risks; and designing for machine learning practitioners without extensive expertise. To accomplish these goals, the team 
										will (i) better understand current practices around AutoML through user studies and interviews; (ii) empirically and analytically explore the 
										security vulnerabilities of AutoML-generated models through assessing these models on widely used datasets; (iii) analyze the results of the 
										first two activities to develop a comprehensive accounting of underlying factors such as standardization of algorithmic choices in the technology 
										or over-reliance on automated metrics by users; and (iv) developing new principles, methodologies, and tools to mitigate the aforementioned risks. 
										The team will also integrate the work into a number of college courses and conduct public outreach to raise awareness of the role machine learning 
										plays in everyday life.
					                  </p>
					                  
									  </p>
					               </div>
					            </div>
					         </article>

							 <article class="timeline-entry animate-box">
					            <div class="timeline-entry-inner">
					               <div class="timeline-icon color-4">
					                  <i class="icon-pen2"></i>
					               </div>
					               <div class="timeline-label">
					               	<h2><a href="#"><a href="https://www.sony.com/en/SonyInfo/research-award-program/"><font color="blue"></font>Sony Research Award -- Federated Semi-Supervised Learning: Exploiting the Power of Unlabeled Data</font></a>, 2022-2023, $100K</a></h2>
					                  <p style="text-align:justify; text-justify:inter-ideograph;">We propose a novel, adaptive, and general framework for federate semi-supervised learning, which is a new research paradigm aiming to exploit 
										the power of unlabeled data ignored by existing federated supervised learning. Introducing unlabeled data to federated learning brings several new challenges, 
										which limits the applicability of existing models. In this proposal, we define the federated semi-supervised learning problem from the insight of data 
										regularization and analyze the new-raised difficulties. In particular, we propose a novel learning framework, named FedSemi, to introduce the consistency 
										regularization technique into federated learning using a teacher-student model in Task 1. In Task 2, we further propose a new metric to measure the 
										divergence of local model layers. Using the divergence, FedSemi can automatically select layer-level parameters to be uploaded to the server in an adaptive manner. 
										In Task 3, we propose an element-wise aggregation approach to reduce the influence of the noisy gradients in the global model aggregation. 
										Task 4 aims to evaluate the proposed framework on three different real world applications, including image classification, sentiment analysis, and health prediction. 
										This project creates new fundamental knowledge in both federated learning and semi-supervise learning fields and will significantly advance these two fields by 
										developing novel methods and implementing analytic tools.
					                  </p>
					                  
									  </p>
					               </div>
					            </div>
					         </article>
					      </div>
					   </div>
				   </div>
				   <p><br><br></p>
				</div>
			</section>
			
			
		</div><!-- end:colorlib-main -->
	</div><!-- end:container-wrap -->
	</div><!-- end:colorlib-page -->

	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Flexslider -->
	<script src="js/jquery.flexslider-min.js"></script>
	<!-- Owl carousel -->
	<script src="js/owl.carousel.min.js"></script>
	<!-- Counters -->
	<script src="js/jquery.countTo.js"></script>
	
	
	<!-- MAIN JS -->
	<script src="js/main.js"></script>

	</body>
</html>

